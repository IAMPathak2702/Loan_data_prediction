# -*- coding: utf-8 -*-
"""Loan_status_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dOKDOSqQw4eL7Tsz99kw_0WTQfSiJjeb

#Loan data predictions
Loan data prediction project typically involve using machine learning algorithms to analyze historical loan data and make predictions about future loan outcomes. These predictions can include determining the likelihood of a borrower defaulting on a loan, estimating the risk associated with lending to a particular individual or business, and forecasting loan performance metrics such as delinquency rates or loan profitability.

The process typically involves collecting and preprocessing loan data, which may include information such as borrower demographics, credit scores, loan terms, and historical repayment behavior. Various machine learning techniques such as classification, regression, and clustering are then applied to build predictive models based on this data.

These predictive models can help financial institutions make more informed decisions about lending by identifying high-risk borrowers, setting appropriate interest rates, and optimizing loan approval processes. Additionally, they can aid in fraud detection and portfolio management by flagging potentially fraudulent applications and identifying trends in loan performance.

Overall, loan data predictions play a crucial role in mitigating risk, improving decision-making, and enhancing the efficiency of lending operations in the financial industry.

# Installing Tfx
"""

!pip install --upgrade pip

!pip install tfx

!pip uninstall shapely -y

"""### Restart Sesion"""

# Commented out IPython magic to ensure Python compatibility.
import os
import pprint
import tempfile
import urllib

import absl
import tensorflow as tf
import tensorflow_model_analysis as tfma
tf.get_logger().propagate = False
pp = pprint.PrettyPrinter()

from tfx import v1 as tfx
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext

# %load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip

print("TFX verison ==",tfx.__version__)
print("Tensorflow Version ==",tf.__version__)

"""# Set filepaths"""

PIPELINE_NAME = "Loan_status_prediction_pipeline"

# This is the root directory for your TFX pip package installation.
_tfx_root = tfx.__path__[0]

# This is the directory containing the TFX Chicago Taxi Pipeline example.
_loan_root = os.path.join(_tfx_root, 'examples/Loan_data_pipelines')

# This is the path where your model will be pushed for serving.
_serving_model_dir = os.path.join(
    tempfile.mkdtemp(), 'serving_model/loan_data_simple')

_pipeline_root = os.path.join(_tfx_root,'pipelines', PIPELINE_NAME)
# Set up logging.
absl.logging.set_verbosity(absl.logging.INFO)

"""# Loading CSV"""

import urllib.request
import tempfile

DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.
_data_url = "https://raw.githubusercontent.com/IAMPathak2702/Loan_data_prediction/main/data.csv"
_data_filepath = os.path.join(DATA_ROOT, "data.csv")
urllib.request.urlretrieve(_data_url, _data_filepath)

!head {_data_filepath}

# Here, we create an InteractiveContext using default parameters. This will
# use a temporary directory with an ephemeral ML Metadata database instance.
# To use your own pipeline root or database, the optional properties
# `pipeline_root` and `metadata_connection_config` may be passed to
# InteractiveContext. Calls to InteractiveContext are no-ops outside of the
# notebook.
context = InteractiveContext()

example_gen = tfx.components.CsvExampleGen(input_base=DATA_ROOT)
context.run(example_gen, enable_cache=True)

artifact = example_gen.outputs['examples'].get()[0]
print(artifact.split_names, artifact.uri)

# Get the URI of the output artifact representing the training examples, which is a directory
train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')

# Get the list of files in this directory (all compressed TFRecord files)
tfrecord_filenames = [os.path.join(train_uri, name)
                      for name in os.listdir(train_uri)]

# Create a `TFRecordDataset` to read these files
dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type="GZIP")

# Iterate over the first 3 records and decode them.
for tfrecord in dataset.take(3):
  serialized_example = tfrecord.numpy()
  example = tf.train.Example()
  example.ParseFromString(serialized_example)
  pp.pprint(example)

"""# Statistic Gen"""

statistics_gen = tfx.components.StatisticsGen(
    examples=example_gen.outputs['examples'])
context.run(statistics_gen, enable_cache=True)

context.show(statistics_gen.outputs['statistics'])

"""# Schema Gen"""

schema_gen = tfx.components.SchemaGen(
    statistics=statistics_gen.outputs['statistics'],
    infer_feature_shape=False)
context.run(schema_gen, enable_cache=True)

context.show(schema_gen.outputs['schema'])

"""# Example Validator"""

example_validator = tfx.components.ExampleValidator(
    statistics=statistics_gen.outputs['statistics'],
    schema=schema_gen.outputs['schema'])
context.run(example_validator, enable_cache=True)

context.show(example_validator.outputs['anomalies'])

_loan_constants_module_file ="loan_constant.py"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {_loan_constants_module_file}
# 
# # Define the numerical features
# NUMERICAL_FEATURES = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']
# 
# # Define the categorical numerical features
# CATEGORICAL_NUMERICAL_FEATURES = ['Dependents']
# 
# # Define the categorical string features
# CATEGORICAL_STRING_FEATURES = ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area']
# 
# # Number of vocabulary terms used for encoding categorical features.
# VOCAB_SIZE = 1000
# 
# # Count of out-of-vocab buckets in which unrecognized categorical are hashed.
# OOV_SIZE = 10
# 
# LABEL_KEY = 'Loan_Status'

_loan_transform_module_file = 'loan_transform.py'

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {_loan_transform_module_file}
# 
# 
# import tensorflow as tf
# import tensorflow_transform as tft
# import loan_constant
# import importlib
# 
# importlib.reload(loan_constant)
# 
# NUMERICAL_FEATURES = loan_constant.NUMERICAL_FEATURES
# CATEGORICAL_NUMERICAL_FEATURES=loan_constant.CATEGORICAL_NUMERICAL_FEATURES
# CATEGORICAL_STRING_FEATURES=loan_constant.CATEGORICAL_STRING_FEATURES
# LABEL_KEY=loan_constant.LABEL_KEY
# VOCAB_SIZE = loan_constant.VOCAB_SIZE
# OOV_SIZE = loan_constant.OOV_SIZE
# LABEL_KEY = loan_constant.LABEL_KEY
# 
# def t_name(key):
#   """
#   Rename the feature keys so that they don't clash with the raw keys when
#   running the Evaluator component.
#   Args:
#     key: The original feature key
#   Returns:
#     key with '_xf' appended
#   """
#   return key + '_xf'
# 
# 
# def _make_one_hot(x, key):
#     """Make a one-hot tensor to encode categorical features."""
#     integerized = tft.compute_and_apply_vocabulary(x,
#                                                    top_k=VOCAB_SIZE,
#                                                    num_oov_buckets=OOV_SIZE,
#                                                    vocab_filename=key,
#                                                    name=key)
#     depth = (tft.experimental.get_vocabulary_size_by_name(key) + OOV_SIZE)
#     one_hot_encoded = tf.one_hot(integerized,
#                                  depth=tf.cast(depth, tf.int32),
#                                  on_value=1.0,
#                                  off_value=0.0)
#     return tf.reshape(one_hot_encoded, [-1, depth])
# 
# def _fill_in_missing(x):
#     """Replace missing values in a SparseTensor."""
#     if not isinstance(x, tf.sparse.SparseTensor):
#         return x
# 
#     default_value = '' if x.dtype == tf.string else 0
#     return tf.squeeze(tf.sparse.to_dense(tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]), default_value), axis=1)
# 
# def preprocessing_fn(inputs):
#     """tf.transform's callback function for preprocessing inputs."""
#     outputs = {}
# 
#     # Drop 'Loan_ID' column
#     inputs.pop('Loan_ID')
# 
#     # Preprocess numerical features
#     for key in NUMERICAL_FEATURES:
#         outputs[t_name(key)] = tft.scale_to_z_score(_fill_in_missing(inputs[key]), name=key)
# 
#     # Preprocess categorical string features
#     for key in CATEGORICAL_STRING_FEATURES:
#         outputs[t_name(key)] = _make_one_hot(_fill_in_missing(inputs[key]), key)
# 
#     # Preprocess categorical numerical features
#     for key in CATEGORICAL_NUMERICAL_FEATURES:
#         outputs[t_name(key)] = _make_one_hot(tf.strings.strip(tf.strings.as_string(_fill_in_missing(inputs[key]))), key)
# 
#     # Process label key
#      # Process label key and cast to tf.int64
#     outputs[LABEL_KEY] = tf.cast(tf.where(tf.equal(_fill_in_missing(inputs[LABEL_KEY]), 'Y'), 1, 0), tf.int64)
# 
#     return outputs
# 
# 
# 
#

transform = tfx.components.Transform(
    examples=example_gen.outputs['examples'],
    schema=schema_gen.outputs['schema'],
    module_file=os.path.abspath(_loan_transform_module_file))
context.run(transform, enable_cache=True)

transform.outputs

# Get the URI of the output artifact representing the transformed examples, which is a directory
train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')

# Get the list of files in this directory (all compressed TFRecord files)
tfrecord_filenames = [os.path.join(train_uri, name)
                      for name in os.listdir(train_uri)]

# Create a `TFRecordDataset` to read these files
dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type="GZIP")

# Iterate over the first 3 records and decode them.
for tfrecord in dataset.take(3):
  serialized_example = tfrecord.numpy()
  example = tf.train.Example()
  example.ParseFromString(serialized_example)
  pp.pprint(example)

"""# Trainer"""

_loan_trainer_module_file = 'loan_trainer.py'

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {_loan_trainer_module_file}
# 
# from typing import Dict, List, Text
# 
# import os
# import glob
# from absl import logging
# 
# import datetime
# import tensorflow as tf
# import tensorflow_transform as tft
# 
# from tfx import v1 as tfx
# from tfx_bsl.public import tfxio
# from tensorflow_transform import TFTransformOutput
# import loan_constant
# 
# import importlib
# importlib.reload(loan_constant)
# 
# _LABEL_KEY = loan_constant.LABEL_KEY
# 
# _BATCH_SIZE = 40
# 
# # Count of out-of-vocab buckets in which unrecognized categorical are hashed.
# 
# 
# 
# 
# def _input_fn(file_pattern: List[Text],
#               data_accessor: tfx.components.DataAccessor,
#               tf_transform_output: tft.TFTransformOutput,
#               batch_size: int = 200) -> tf.data.Dataset:
#   """Generates features and label for tuning/training.
# 
#   Args:
#     file_pattern: List of paths or patterns of input tfrecord files.
#     data_accessor: DataAccessor for converting input to RecordBatch.
#     tf_transform_output: A TFTransformOutput.
#     batch_size: representing the number of consecutive elements of returned
#       dataset to combine in a single batch
# 
#   Returns:
#     A dataset that contains (features, indices) tuple where features is a
#       dictionary of Tensors, and indices is a single Tensor of label indices.
#   """
#   return data_accessor.tf_dataset_factory(
#       file_pattern,
#       tfxio.TensorFlowDatasetOptions(
#           batch_size=batch_size, label_key=_LABEL_KEY),
#       tf_transform_output.transformed_metadata.schema)
# 
# def _get_tf_examples_serving_signature(model, tf_transform_output):
#   """Returns a serving signature that accepts `tensorflow.Example`."""
# 
#   # You need to track the layers in the model in order to save it.
#   model.tft_layer_inference = tf_transform_output.transform_features_layer()
# 
#   @tf.function(input_signature=[
#       tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')
#   ])
#   def serve_tf_examples_fn(serialized_tf_example):
#     """Returns the output to be used in the serving signature."""
#     raw_feature_spec = tf_transform_output.raw_feature_spec()
#     # Remove label feature since these will not be present at serving time.
#     raw_feature_spec.pop(_LABEL_KEY)
#     raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)
#     transformed_features = model.tft_layer_inference(raw_features)
#     logging.info('serve_transformed_features = %s', transformed_features)
# 
#     outputs = model(transformed_features)
#     return {'outputs': outputs}
# 
#   return serve_tf_examples_fn
# 
# 
# def _get_transform_features_signature(model, tf_transform_output):
#   """Returns a serving signature that applies tf.Transform to features."""
# 
#   # You need to track the layers in the model in order to save it.
#   model.tft_layer_eval = tf_transform_output.transform_features_layer()
# 
#   @tf.function(input_signature=[
#       tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')
#   ])
#   def transform_features_fn(serialized_tf_example):
#     """Returns the transformed_features to be fed as input to evaluator."""
#     raw_feature_spec = tf_transform_output.raw_feature_spec()
#     raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)
#     transformed_features = model.tft_layer_eval(raw_features)
#     logging.info('eval_transformed_features = %s', transformed_features)
#     return transformed_features
# 
#   return transform_features_fn
# 
# 
# def export_serving_model(tf_transform_output, model, output_dir):
#   """Exports a keras model for serving.
#   Args:
#     tf_transform_output: Wrapper around output of tf.Transform.
#     model: A keras model to export for serving.
#     output_dir: A directory where the model will be exported to.
#   """
#   # The layer has to be saved to the model for keras tracking purpases.
#   model.tft_layer = tf_transform_output.transform_features_layer()
# 
#   signatures = {
#       'serving_default':
#           _get_tf_examples_serving_signature(model, tf_transform_output),
#       'transform_features':
#           _get_transform_features_signature(model, tf_transform_output),
#   }
# 
#   model.save(output_dir, save_format='tf', signatures=signatures)
# 
# 
# def _build_keras_model(tf_transform_output: TFTransformOutput
#                        ) -> tf.keras.Model:
#   """Creates a DNN Keras model for classifying taxi data.
# 
#   Args:
#     tf_transform_output: [TFTransformOutput], the outputs from Transform
# 
#   Returns:
#     A keras Model.
#   """
#   feature_spec = tf_transform_output.transformed_feature_spec().copy()
#   feature_spec.pop(_LABEL_KEY)
# 
#   inputs = {}
#   for key, spec in feature_spec.items():
#     if isinstance(spec, tf.io.VarLenFeature):
#       inputs[key] = tf.keras.layers.Input(
#           shape=[None], name=key, dtype=spec.dtype, sparse=True)
#     elif isinstance(spec, tf.io.FixedLenFeature):
#       inputs[key] = tf.keras.layers.Input(
#           shape=spec.shape or [1], name=key, dtype=spec.dtype)
#     else:
#       raise ValueError('Spec type is not supported: ', key, spec)
# 
#   output = tf.keras.layers.Concatenate()(tf.nest.flatten(inputs))
#   output = tf.keras.layers.Dense(100, activation='relu')(output)
#   output = tf.keras.layers.Dense(70, activation='relu')(output)
#   output = tf.keras.layers.Dense(50, activation='relu')(output)
#   output = tf.keras.layers.Dense(20, activation='relu')(output)
#   output = tf.keras.layers.Dense(1)(output)
#   return tf.keras.Model(inputs=inputs, outputs=output)
# 
# 
# # TFX Trainer will call this function.
# def run_fn(fn_args: tfx.components.FnArgs):
#   """Train the model based on given args.
# 
#   Args:
#     fn_args: Holds args used to train the model as name/value pairs.
#   """
#   tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)
# 
#   train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor,
#                             tf_transform_output, _BATCH_SIZE)
#   eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor,
#                            tf_transform_output, _BATCH_SIZE)
# 
# 
# 
#   model = _build_keras_model(tf_transform_output)
# 
#   model.compile(
#       loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
#       optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
#       metrics=[tf.keras.metrics.BinaryAccuracy()])
# 
#   tensorboard_callback = tf.keras.callbacks.TensorBoard(
#       log_dir=fn_args.model_run_dir, update_freq='batch')
# 
#   model.fit(
#       train_dataset,
#       steps_per_epoch=fn_args.train_steps,
#       validation_data=eval_dataset,
#       validation_steps=fn_args.eval_steps,
#       callbacks=[tensorboard_callback])
# 
#   # Export the model.
#   export_serving_model(tf_transform_output, model, fn_args.serving_model_dir)

trainer = tfx.components.Trainer(
    module_file=os.path.abspath(_loan_trainer_module_file),
    examples=transform.outputs['transformed_examples'],
    transform_graph=transform.outputs['transform_graph'],
    schema=schema_gen.outputs['schema'],
    train_args=tfx.proto.TrainArgs(num_steps=10000),
    eval_args=tfx.proto.EvalArgs(num_steps=5000))
context.run(trainer, enable_cache=True)

"""# Analyze training with TensorBoard"""

model_artifact_dir = trainer.outputs['model'].get()[0].uri
pp.pprint(os.listdir(model_artifact_dir))
model_dir = os.path.join(model_artifact_dir, 'Format-Serving')
pp.pprint(os.listdir(model_dir))

# Commented out IPython magic to ensure Python compatibility.
model_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri

# %load_ext tensorboard
# %tensorboard --logdir {model_run_artifact_dir}

"""# Evaluator"""

import loan_constant
import sys

import importlib
importlib.reload(loan_constant)

eval_config = tfma.EvalConfig(
    model_specs=[
        # This assumes a serving model with signature 'serving_default'. If
        # using estimator based EvalSavedModel, add signature_name: 'eval' and
        # remove the label_key.
        tfma.ModelSpec(
            signature_name='serving_default',
            label_key=loan_constant.LABEL_KEY,
            preprocessing_function_names=['transform_features'],
            )
        ],
    metrics_specs=[
        tfma.MetricsSpec(
            # The metrics added here are in addition to those saved with the
            # model (assuming either a keras model or EvalSavedModel is used).
            # Any metrics added into the saved model (for example using
            # model.compile(..., metrics=[...]), etc) will be computed
            # automatically.
            # To add validation thresholds for metrics saved with the model,
            # add them keyed by metric name to the thresholds map.
            metrics=[
                tfma.MetricConfig(class_name='ExampleCount'),
                tfma.MetricConfig(class_name='BinaryAccuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          lower_bound={'value': 0.5}),
                      # Change threshold will be ignored if there is no
                      # baseline model resolved from MLMD (first run).
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-10})))
            ]
        )
    ],
    slicing_specs=[
        # An empty slice spec means the overall slice, i.e. the whole dataset.
        tfma.SlicingSpec(),
        # Data can be sliced along a feature column. In this case, data is
        # sliced along feature column trip_start_hour.
        tfma.SlicingSpec(
            feature_keys=['ApplicantIncome'])
    ])

model_resolver = tfx.dsl.Resolver(
      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,
      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),
      model_blessing=tfx.dsl.Channel(
          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(
              'latest_blessed_model_resolver')
context.run(model_resolver, enable_cache=True)

evaluator = tfx.components.Evaluator(
    examples=example_gen.outputs['examples'],
    model=trainer.outputs['model'],
    baseline_model=model_resolver.outputs['model'],
    eval_config=eval_config)
context.run(evaluator, enable_cache=True)

evaluator.outputs

context.show(evaluator.outputs['evaluation'])

import tensorflow_model_analysis as tfma

# Get the TFMA output result path and load the result.
PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri
tfma_result = tfma.load_eval_result(PATH_TO_RESULT)

# Show data sliced along feature column trip_start_hour.
tfma.view.render_slicing_metrics(
    tfma_result, slicing_column='ApplicantIncome')

blessing_uri = evaluator.outputs['blessing'].get()[0].uri
!ls -l {blessing_uri}

PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri
print(tfma.load_validation_result(PATH_TO_RESULT))

"""# Pusher"""

pusher = tfx.components.Pusher(
    model=trainer.outputs['model'],
    model_blessing=evaluator.outputs['blessing'],
    push_destination=tfx.proto.PushDestination(
        filesystem=tfx.proto.PushDestination.Filesystem(
            base_directory=_serving_model_dir)))
context.run(pusher, enable_cache=True)

pusher.outputs